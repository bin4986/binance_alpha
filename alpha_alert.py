#!/usr/bin/env python3
# -*- coding: utf-8 -*-

import os, re, json, time
from pathlib import Path
from typing import Dict, List, Any, Optional
import requests

ALPHA_URLS = [
    "https://www.binance.com/en/feed/alpha",
    "https://www.binance.com/ko/feed/alpha",
]

TIMEOUT = 20
SEEN_FILE = Path("seen_ids.json")
TG_TOKEN = os.getenv("TELEGRAM_BOT_TOKEN", "")
TG_CHAT_ID = os.getenv("TELEGRAM_CHAT_ID", "")

DEFAULT_HEADERS = {
    "User-Agent": "Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120 Safari/537.36",
    "Accept": "text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8",
    "Accept-Language": "en-US,en;q=0.9,ko;q=0.8",
    "Referer": "https://www.binance.com/en",
    "Origin": "https://www.binance.com",
}

LISTING_KEYWORDS = [
    "listing", "listed", "new listing", "lists",
    "ìƒì¥", "ê±°ë˜ ê°œì‹œ", "ì…ê¸ˆ", "ìƒì¥ ì•ˆë‚´"
]

RE_EVM = re.compile(r"\b0x[a-fA-F0-9]{40}\b")
RE_TW  = re.compile(r"https?://(?:www\.)?twitter\.com/[A-Za-z0-9_]+", re.IGNORECASE)
RE_SOL = re.compile(r"\b[1-9A-HJ-NP-Za-km-z]{32,44}\b")
RE_NEXT_DATA = re.compile(rb'id="__NEXT_DATA__"[^>]*>\s*({.*?})\s*</script>', re.DOTALL)
RE_APP_DATA  = re.compile(rb'id="__APP_DATA"[^>]*>\s*({.*?})\s*</script>', re.DOTALL)
RE_DATA_STATE= re.compile(rb'data-state="([^"]+)"')  # ì¼ë¶€ í˜ì´ì§€ëŠ” data-stateì— JSONì„ ë‹´ìŒ(escape ì£¼ì˜)

def http_get(url: str) -> bytes:
    r = requests.get(url, headers=DEFAULT_HEADERS, timeout=TIMEOUT)
    r.raise_for_status()
    return r.content

def _json_from_candidates(html: bytes) -> Optional[dict]:
    # 1) __NEXT_DATA__
    m = RE_NEXT_DATA.search(html)
    if m:
        try:
            return json.loads(m.group(1))
        except Exception:
            pass
    # 2) __APP_DATA
    m = RE_APP_DATA.search(html)
    if m:
        try:
            return json.loads(m.group(1))
        except Exception:
            pass
    # 3) data-state (html-escaped)
    m = RE_DATA_STATE.search(html)
    if m:
        raw = m.group(1)
        try:
            # HTML ì—”í‹°í‹° ë””ì½”ë”©
            s = raw.decode("utf-8")
            s = s.replace("&quot;", '"').replace("&amp;", "&").replace("&#x27;", "'")
            return json.loads(s)
        except Exception:
            pass
    return None

def looks_like_listing(text: str) -> bool:
    t = (text or "").lower()
    return any(k in t for k in LISTING_KEYWORDS)

def scrape_alpha_feed(pages: int = 1) -> List[Dict[str, Any]]:
    """
    Alpha í”¼ë“œ HTMLì˜ ì´ˆê¸° ìƒíƒœ(JSON)ì—ì„œ ê¸€ ëª©ë¡ ì¶”ì¶œ.
    í˜ì´ì§€ ë§¤ê¹€ì€ í´ë¼ì´ì–¸íŠ¸ ì‚¬ì´ë“œì¸ ê²½ìš°ê°€ ë§ì•„ì„œ,
    ì—¬ëŸ¬ ì–¸ì–´ URLì„ ì‹œë„í•˜ê³ , JSON íŠ¸ë¦¬ì—ì„œ article list ë¹„ìŠ·í•œ í‚¤ë“¤ì„ í­ë„“ê²Œ íƒìƒ‰.
    """
    results: List[Dict[str, Any]] = []

    def pick_from_tree(obj: Any):
        # íŠ¸ë¦¬ ì „ì²´ë¥¼ ë„ëŠ” ì œë„¤ë¦­ íƒìƒ‰: article-like dictë“¤ì„ ìˆ˜ì§‘
        if isinstance(obj, dict):
            # ëŒ€í‘œì ìœ¼ë¡œ id/title/brief ì¡°í•©ì„ ê°€ì§„ ë…¸ë“œ ì¶”ì¶œ
            if ("id" in obj or "articleId" in obj or "code" in obj) and ("title" in obj or "brief" in obj or "summary" in obj):
                aid = str(obj.get("id") or obj.get("articleId") or obj.get("code"))
                title = (obj.get("title") or "").strip()
                brief = (obj.get("brief") or obj.get("summary") or "").strip()
                if aid and (looks_like_listing(title) or looks_like_listing(brief)):
                    results.append({
                        "id": aid,
                        "title": title,
                        "brief": brief,
                        "release": obj.get("releaseDate") or obj.get("ctime") or ""
                    })
            for v in obj.values():
                pick_from_tree(v)
        elif isinstance(obj, list):
            for v in obj:
                pick_from_tree(v)

    # ì—¬ëŸ¬ URL í›„ë³´ë¥¼ ì‹œë„
    for url in ALPHA_URLS:
        try:
            html = http_get(url)
        except Exception as e:
            print(f"[warn] alpha GET failed: {url} {e}")
            continue
        data = _json_from_candidates(html)
        if not data:
            print(f"[warn] no JSON state found on {url[:60]}...")
            continue
        pick_from_tree(data)
        if results:
            break

    # ì¤‘ë³µ ì œê±°(ê°™ì€ ê¸€ì´ ì—¬ëŸ¬ íŠ¸ë¦¬ ê²½ë¡œì—ì„œ ë°œê²¬ë  ìˆ˜ ìˆìŒ)
    uniq: Dict[str, Dict[str, Any]] = {}
    for a in results:
        uniq[a["id"]] = a
    return list(uniq.values())

def scrape_alpha_detail(article_id: str) -> str:
    # ìƒì„¸ëŠ” /en/feed/post/<id> í˜•íƒœ í˜ì´ì§€ì—ì„œ ë³¸ë¬¸ í…ìŠ¤íŠ¸ë¥¼ JSON ìƒíƒœë¡œ ì–»ëŠ” ì‹ìœ¼ë¡œ ì²˜ë¦¬
    urls = [
        f"https://www.binance.com/en/feed/post/{article_id}",
        f"https://www.binance.com/ko/feed/post/{article_id}",
    ]
    for url in urls:
        try:
            html = http_get(url)
            data = _json_from_candidates(html)
            if not data:
                continue
            # ë³¸ë¬¸ í…ìŠ¤íŠ¸/HTML ë¹„ìŠ·í•œ í‚¤ë¥¼ í­ë„“ê²Œ íƒìƒ‰
            content = ""

            def find_content(o: Any):
                nonlocal content
                if content:
                    return
                if isinstance(o, dict):
                    # í”í•œ í‚¤ ì´ë¦„ë“¤
                    for k in ("content", "body", "html", "md", "markdown", "richText"):
                        if k in o and isinstance(o[k], str) and len(o[k]) > 20:
                            content = o[k]
                            return
                    for v in o.values():
                        find_content(v)
                elif isinstance(o, list):
                    for v in o:
                        find_content(v)

            find_content(data)
            if content:
                return content
        except Exception as e:
            print(f"[warn] detail GET failed: {url} {e}")
            continue
    return ""

def extract_refs(text: str) -> Dict[str, List[str]]:
    if not text:
        return {"evm": [], "sol": [], "twitter": []}
    evm = list(dict.fromkeys(RE_EVM.findall(text)))
    sol = list(dict.fromkeys(RE_SOL.findall(text)))
    tw  = list(dict.fromkeys(RE_TW.findall(text)))
    return {"evm": evm, "sol": sol, "twitter": tw}

def send_telegram(msg: str) -> None:
    if not TG_TOKEN or not TG_CHAT_ID:
        print("âš ï¸ TELEGRAM ENV not set; printing message:\n", msg)
        return
    url = f"https://api.telegram.org/bot{TG_TOKEN}/sendMessage"
    payload = {"chat_id": TG_CHAT_ID, "text": msg, "parse_mode": "HTML", "disable_web_page_preview": True}
    r = requests.post(url, json=payload, timeout=TIMEOUT)
    r.raise_for_status()

def format_message(a: Dict[str, Any], refs: Dict[str, List[str]]) -> str:
    title = a.get("title", "").strip()
    aid = a.get("id")
    link = f"https://www.binance.com/en/feed/post/{aid}"
    lines = [
        "ğŸŸ¡ <b>Binance Alpha: New Listing</b>",
        f"ğŸ“° <b>{title}</b>",
        f"ğŸ”— <a href='{link}'>Alpha Post</a>",
    ]
    if refs["evm"]:
        lines.append("ğŸ§¾ <b>Contracts</b>\n" + "\n".join(f"â€¢ <code>{c}</code>" for c in refs["evm"][:6]))
    if refs["sol"]:
        lines.append("ğŸ§¾ <b>Solana-like Keys</b>\n" + "\n".join(f"â€¢ <code>{c}</code>" for c in refs["sol"][:6]))
    if refs["twitter"]:
        lines.append("ğŸ¦ <b>Twitter</b>\n" + "\n".join(f"â€¢ {u}" for u in refs["twitter"][:5]))
    return "\n".join(lines)

def load_seen() -> set:
    if SEEN_FILE.exists():
        try:
            return set(json.loads(SEEN_FILE.read_text(encoding="utf-8")))
        except Exception:
            return set()
    return set()

def save_seen(seen: set) -> None:
    SEEN_FILE.write_text(json.dumps(sorted(list(seen)), ensure_ascii=False), encoding="utf-8")

def process_once(pages: int = 1) -> int:
    seen = load_seen()
    sent = 0
    articles = scrape_alpha_feed(pages)
    # ìµœì‹ ë§Œ ë˜ì§€ëŠ” ì •ì±… ìœ ì§€
    for a in articles:
        aid = a["id"]
        if aid in seen:
            continue
        content = scrape_alpha_detail(aid)
        refs = extract_refs(content)
        msg = format_message(a, refs)
        try:
            send_telegram(msg)
            seen.add(aid)
            sent += 1
        except Exception as e:
            print(f"[error] telegram send failed for {aid}: {e}")
    save_seen(seen)
    return sent

def main():
    pages = int(os.getenv("PAGES", "1"))
    sent = process_once(pages)
    print(f"done. sent={sent}")

if __name__ == "__main__":
    main()
