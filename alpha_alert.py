# alpha_alert.py
import os, json, re, time
from pathlib import Path

import requests
from bs4 import BeautifulSoup

# ================== ì„¤ì • ==================
# ë°”ì´ë‚¸ìŠ¤ ì•ŒíŒŒ(ìƒì¥/ë¦¬ìŠ¤íŠ¸) í˜ì´ì§€ URL â€” í•„ìš”ì‹œ ì •í™• ì£¼ì†Œë¡œ ë°”ê¿” ì¨
ALPHA_URL = "https://www.binance.com/en/feed/alpha"   # ì˜ˆì‹œ: ìƒì¥ ê´€ë ¨ ê¸€ì´ ëª¨ì´ëŠ” í”¼ë“œ
TIMEOUT = 20
SEEN_FILE = Path("seen_ids.json")
HEADERS = {
    "User-Agent": "Mozilla/5.0 (compatible; AlphaWatcher/1.0; +https://github.com/)"
}
# ========================================

TOKEN = (os.getenv("TELEGRAM_BOT_TOKEN") or "").strip()
CHAT_ID = (os.getenv("TELEGRAM_CHAT_ID") or "").strip()

def send_telegram(text: str):
    if not TOKEN or not CHAT_ID:
        raise RuntimeError("TELEGRAM_BOT_TOKEN / TELEGRAM_CHAT_ID í™˜ê²½ë³€ìˆ˜ê°€ ì—†ìŠµë‹ˆë‹¤.")
    url = f"https://api.telegram.org/bot{TOKEN}/sendMessage"
    r = requests.post(url, data={"chat_id": CHAT_ID, "text": text, "disable_web_page_preview": True})
    # ë””ë²„ê·¸ìš© ì¶œë ¥
    print("Telegram API response (python):", r.text)
    r.raise_for_status()

def load_seen() -> set:
    if SEEN_FILE.exists():
        try:
            return set(json.loads(SEEN_FILE.read_text(encoding="utf-8")))
        except Exception:
            return set()
    return set()

def save_seen(seen: set):
    SEEN_FILE.write_text(json.dumps(sorted(seen)), encoding="utf-8")

def get_soup(url: str) -> BeautifulSoup:
    r = requests.get(url, headers=HEADERS, timeout=TIMEOUT)
    r.raise_for_status()
    return BeautifulSoup(r.text, "html.parser")

def extract_contracts(html: str) -> list[str]:
    """EVM ê³„ì—´ ì»¨íŠ¸ë™íŠ¸(0x + 40hex) ì£¼ì†Œ íƒì§€ + ìŠ¤ìºë„ˆ ë§í¬ ì¶”ì¶œ"""
    addrs = set(re.findall(r"(0x[a-fA-F0-9]{40})", html))
    # ìŠ¤ìºë„ˆë¥˜ ë§í¬(etherscan/bscscan/arbitrum/polygonscan ë“±)ë„ ê°™ì´ ì°¾ì•„ì„œ ë¶™ì„
    scan_links = re.findall(r'https?://(?:www\.)?(?:etherscan|bscscan|arbiscan|polygonscan|snowtrace|basescan|optimistic\.etherscan)\.[^\s"\'<>]+', html)
    return sorted(addrs.union(scan_links))

def extract_twitter_links(soup: BeautifulSoup) -> list[str]:
    links = []
    for a in soup.select('a[href*="twitter.com/"]'):
        href = a.get("href")
        if href and "intent/tweet" not in href:
            links.append(href)
    return sorted(set(links))

def parse_feed_items(soup: BeautifulSoup):
    """
    í”¼ë“œì—ì„œ ê¸€ ì¹´ë“œë“¤ íŒŒì‹±.
    ê° ì¹´ë“œì— ëŒ€í•´ (id, title, url) ë°˜í™˜.
    ì‚¬ì´íŠ¸ êµ¬ì¡°ê°€ ìì£¼ ë°”ë€” ìˆ˜ ìˆì–´ ë„ë„í•˜ê²Œ ì¡ìŒ.
    """
    items = []
    # a íƒœê·¸ ì¤‘ ê²Œì‹œê¸€/ìƒì„¸ë¡œ ë³´ì´ëŠ” ê²ƒë“¤ ìˆ˜ì§‘
    for a in soup.select('a[href*="/en/feed/"]'):
        href = a.get("href", "")
        title = (a.get_text(strip=True) or "")[:200]
        if not href or "/en/feed/" not in href:
            continue
        # ì ˆëŒ€ URLë¡œ ë³€í™˜
        if href.startswith("/"):
            url = "https://www.binance.com" + href
        else:
            url = href
        # ê°„ë‹¨ id: ìƒì„¸ URL ì „ì²´ë¥¼ idë¡œ ì‚¬ìš©
        post_id = url
        # ì œëª©ì— listing/new/announce ê°™ì€ í‚¤ì›Œë“œê°€ ìˆëŠ” ê²ƒë§Œ ìš°ì„  í•„í„° (ëŠìŠ¨í•˜ê²Œ)
        kw = title.lower()
        if any(k in kw for k in ["list", "listing", "launch", "announcement", "adds", "trading", "pair"]):
            items.append((post_id, title, url))
    # ì¤‘ë³µ ì œê±°
    uniq = {}
    for pid, t, u in items:
        uniq[pid] = (pid, t, u)
    return list(uniq.values())

def enrich_details(url: str):
    """ìƒì„¸ í˜ì´ì§€ ë“¤ì–´ê°€ì„œ íŠ¸ìœ„í„°/ì»¨íŠ¸ë™íŠ¸ ë“± ì¶”ê°€ ìˆ˜ì§‘"""
    soup = get_soup(url)
    html = str(soup)
    tws = extract_twitter_links(soup)
    contracts = extract_contracts(html)

    # ì œëª©/ì‹¬ë³¼ í›„ë³´
    title = soup.title.get_text(strip=True) if soup.title else url
    # ì‹¬ë³¼ì€ ( ... (XYZ) ) íŒ¨í„´ ê°™ì€ ê±¸ ëŠìŠ¨í•˜ê²Œ ìºì¹˜
    m = re.search(r"\(([A-Z0-9]{2,10})\)", title)
    symbol = m.group(1) if m else None

    return {
        "title": title,
        "symbol": symbol,
        "twitters": tws[:3],         # ë„ˆë¬´ ë§ìœ¼ë©´ 3ê°œê¹Œì§€
        "contracts": contracts[:5],  # 5ê°œê¹Œì§€
    }

def make_message(title: str, url: str, symbol: str | None, tws: list[str], contracts: list[str]) -> str:
    lines = []
    lines.append("ğŸŸ¡ *Binance Alpha â€“ New Listing Detected*")
    lines.append(f"â€¢ Title: {title}")
    if symbol:
        lines.append(f"â€¢ Symbol: {symbol}")
    lines.append(f"â€¢ Source: {url}")
    if tws:
        lines.append("â€¢ Twitter: " + ", ".join(tws))
    if contracts:
        lines.append("â€¢ Contract: " + ", ".join(contracts))
    return "\n".join(lines)

def main():
    seen = load_seen()
    feed = get_soup(ALPHA_URL)
    candidates = parse_feed_items(feed)

    new_count = 0
    for pid, title, url in candidates:
        if pid in seen:
            continue
        # ìƒì„¸ ë“¤ì–´ê°€ì„œ ë³´ê°• í›„ â€˜ìƒì¥â€™ í‚¤ì›Œë“œì¸ì§€ ì¬í™•ì¸(ì—¬ê¸°ì„  ëŠìŠ¨)
        try:
            detail = enrich_details(url)
        except Exception as e:
            print(f"[warn] detail fetch failed for {url}: {e}")
            continue

        msg = make_message(
            title=detail.get("title") or title,
            url=url,
            symbol=detail.get("symbol"),
            tws=detail.get("twitters", []),
            contracts=detail.get("contracts", []),
        )
        # ì „ì†¡
        send_telegram(msg)
        # ë„ˆë¬´ ë¹ ë¥¸ ì—°ì† í˜¸ì¶œ íšŒí”¼
        time.sleep(1.5)

        seen.add(pid)
        new_count += 1

    # ìƒˆ ê¸€ì´ ìˆì„ ë•Œë§Œ ê¸°ë¡ ê°±ì‹ 
    if new_count > 0:
        save_seen(seen)
        print(f"[info] sent {new_count} new alerts")
    else:
        print("[info] no new listing â€” no alert sent")

if __name__ == "__main__":
    main()
