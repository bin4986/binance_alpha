import os, time, json, re, html
import requests
from urllib.parse import urljoin, urlencode
from bs4 import BeautifulSoup
from dotenv import load_dotenv

load_dotenv()

BOT_TOKEN = os.getenv("TELEGRAM_BOT_TOKEN")
CHAT_ID = os.getenv("TELEGRAM_CHAT_ID")
INTERVAL = int(os.getenv("CHECK_INTERVAL_SECONDS", "900"))

# Binance CMS API (New Cryptocurrency Listing = catalogId 48)
BINANCE_API = "https://www.binance.com/bapi/composite/v1/public/cms/article/list/query"
CATALOG_ID = 48  # New Cryptocurrency Listing

SEEN_FILE = "seen_alpha_ids.json"

# ì²´ì¸ë³„ ìµìŠ¤í”Œë¡œëŸ¬ (í•„ìš” ì‹œ ì¶”ê°€)
EXPLORERS = {
    "ETH": "https://etherscan.io/address/{addr}",
    "BNB": "https://bscscan.com/address/{addr}",
    "Base": "https://basescan.org/address/{addr}",
    "Solana": "https://solscan.io/token/{addr}",
    "Sui": "https://suiscan.xyz/mainnet/object/{addr}",
    "TON": "https://tonviewer.com/{addr}",
    "Arbitrum": "https://arbiscan.io/address/{addr}",
    "Polygon": "https://polygonscan.com/address/{addr}",
}

HEADERS = {
    "User-Agent": "AlphaAlertBot/1.0 (+https://binance.com)"
}

def tg_send(text):
    url = f"https://api.telegram.org/bot{BOT_TOKEN}/sendMessage"
    requests.post(url, data={"chat_id": CHAT_ID, "text": text, "disable_web_page_preview": True})

def load_seen():
    if os.path.exists(SEEN_FILE):
        try:
            with open(SEEN_FILE, "r", encoding="utf-8") as f:
                return set(json.load(f))
        except Exception:
            return set()
    return set()

def save_seen(seen_ids):
    with open(SEEN_FILE, "w", encoding="utf-8") as f:
        json.dump(list(seen_ids), f, ensure_ascii=False)

def fetch_new_listing_articles(page_no=1, page_size=30):
    params = {"type": 1, "pageNo": page_no, "pageSize": page_size, "catalogId": CATALOG_ID}
    r = requests.get(BINANCE_API, params=params, headers=HEADERS, timeout=20)
    r.raise_for_status()
    data = r.json()
    return data.get("data", {}).get("articles", []) or []

def is_alpha_new_listing(title: str) -> bool:
    t = (title or "").lower()
    if "will be available on binance alpha" in t:
        return True
    # ì—¬ì§€ëŠ” ë‚¨ê²¨ë‘ë˜, â€œalpha listingâ€ ë‰˜ì•™ìŠ¤ë§Œ ì¡ê³  promos/editsëŠ” ì œì™¸
    if "binance alpha" in t and "will be available" in t:
        return True
    return False

def is_noise(title: str) -> bool:
    t = (title or "").lower()
    # í”„ë¡œëª¨ì…˜/ìˆ˜ì •/ë¹„ìƒì¥ì„± ê¸€ í•„í„°
    noise_keywords = ["promotion", "promo", "fee", "update", "amend", "adjustment", "contest", "campaign"]
    return any(k in t for k in noise_keywords)

def build_announcement_link(code, slug):
    # ì½”ë“œ/ìŠ¬ëŸ¬ê·¸ ì¡°í•©ìœ¼ë¡œ ìƒì„¸ ê³µì§€ ë§í¬ êµ¬ì„±
    # ìƒˆ í˜•ì‹(â€œ/detail/<code>â€)ì´ ë³´í¸ì ì´ë¯€ë¡œ ìš°ì„  ì‚¬ìš©
    base = "https://www.binance.com/en/support/announcement/detail/"
    return urljoin(base, code) if code else f"https://www.binance.com/en/support/announcement/{slug or ''}"

def get_article_detail_html(link):
    try:
        r = requests.get(link, headers=HEADERS, timeout=20)
        r.raise_for_status()
        return r.text
    except Exception:
        return ""

def extract_official_links_and_contracts(page_html: str):
    """
    - ê³µì§€ ë³¸ë¬¸ì´ë‚˜ â€˜Project Linksâ€™, â€˜Official Linksâ€™ ì„¹ì…˜ì—ì„œ
      X(Twitter)/Website/Docsë¥¼ ìš°ì„  ìˆ˜ì§‘
    - ê³µì‹ ì‚¬ì´íŠ¸ë¥¼ 1~2 hop íŒ”ë¡œìš°í•˜ì—¬ ì»¨íŠ¸ë™íŠ¸ë¥¼ ì¶”ì¶œ(0x..., Solana base58, TON Eq..., Sui 0x...)
    - ë°˜ë“œì‹œ ê³µì‹ ë„ë©”ì¸/ë¬¸ì„œì—ì„œë§Œ ì¶”ì¶œ
    """
    soup = BeautifulSoup(page_html, "html.parser")

    # 1) ê³µì§€ ë‚´ X(Twitter) / Website ë§í¬
    x_handle = None
    official_sites = []

    for a in soup.select("a[href]"):
        href = a["href"].strip()
        text = (a.get_text(strip=True) or "").lower()
        # íŠ¸ìœ„í„° ë§í¬
        if "x.com" in href or "twitter.com" in href:
            x_handle = href
        # ê³µì‹ ì‚¬ì´íŠ¸ í›„ë³´
        if any(k in text for k in ["official site", "website", "project website", "docs"]) or \
           re.search(r"(official|project|website|docs)", text, re.I):
            official_sites.append(href)
        # Sometimes they just list project domain
        if re.match(r"^https?://[A-Za-z0-9\.\-]+/?$", href):
            official_sites.append(href)

    official_sites = list(dict.fromkeys(official_sites))  # dedupe

    # 2) ê³µì§€ ë³¸ë¬¸ì—ì„œ ì»¨íŠ¸ë™íŠ¸ íŒ¨í„´ ì¶”ì¶œ(ì•ˆì •ì„± ìœ„í•´ â€˜ê³µì‹â€™ ë§í¬ì—ì„œë„ë§Œ ì‹œë„)
    #    ë¨¼ì € ê³µì§€ ë³¸ë¬¸ì—ì„œ ì§ì ‘ íŒ¨í„´ ìŠ¤ìº”
    contracts = find_contracts_in_text(soup.get_text(" ", strip=True))

    # 3) ê³µì‹ ì‚¬ì´íŠ¸/ë¬¸ì„œ 1~2ê°œë§Œ ì¶”ì í•´ì„œ ì¶”ê°€ ì¶”ì¶œ (ë„ˆë¬´ ê¹Šê²Œ í¬ë¡¤ë§í•˜ì§€ ì•ŠìŒ)
    for site in official_sites[:2]:
        try:
            rr = requests.get(site, headers=HEADERS, timeout=20)
            txt = rr.text
            # ë³´ì•ˆìƒ: ì™¸ë¶€ ì‚¬ì´íŠ¸ì—ì„œ ì¶”ì¶œí•œ ê±´ "ì°¸ê³ "ë¡œë§Œ, ê³µì§€/ë¬¸ì„œ í˜ì´ì§€ì˜ ëª…ì‹œ êµ¬ê°„ì¼ ë•Œë§Œ ì‚¬ìš© ê¶Œì¥
            contracts.update(find_contracts_in_text(BeautifulSoup(txt, "html.parser").get_text(" ", strip=True)))
            # íŠ¸ìœ„í„° ì¬ë°œê²¬
            if not x_handle:
                m = re.search(r"https?://(x\.com|twitter\.com)/[A-Za-z0-9_]+", txt)
                if m:
                    x_handle = m.group(0)
        except Exception:
            pass

    # ì²´ì¸ë³„ ì£¼ì†Œ ì •ë¦¬(ì¶œì²˜ ë¶ˆëª…í™•ì‹œ ì œì™¸ ê¶Œì¥ â†’ ì—¬ê¸°ì„  ë°œê²¬ë§Œ; ì‹¤ì œ ì•Œë¦¼ ì „ì—ëŠ” â€˜ê³µì‹ ì¶œì²˜â€™ ì—¬ë¶€ íŒë‹¨ í•„ìš”)
    normalized = normalize_contracts(contracts)
    return x_handle, normalized

def find_contracts_in_text(text: str):
    """
    ë¬¸ìì—´ì—ì„œ ì£¼ì†Œ íŒ¨í„´ì„ ì°¾ì•„ ì„ì‹œ ìˆ˜ì§‘.
    ì‹¤ì œ ì‚¬ìš© ì „ 'ê³µì‹ ì¶œì²˜' í™•ì¸ í•„ìš”.
    """
    found = set()
    t = text or ""

    # EVMë¥˜ (ETH/BNB/Base/Arbitrum/Polygon/Suiì˜ 0xâ€¦ í˜•íƒœ) - Suië„ 0xê°€ ë§ìŒ
    for m in re.finditer(r"\b0x[a-fA-F0-9]{38,64}\b", t):
        found.add(m.group(0))

    # Solana: base58 ëŒ€ëµ 32~44ì
    for m in re.finditer(r"\b[1-9A-HJ-NP-Za-km-z]{32,44}\b", t):
        found.add(m.group(0))

    # TON: â€˜EQâ€™ë¡œ ì‹œì‘í•˜ëŠ” ì£¼ì†Œê°€ í”í•¨
    for m in re.finditer(r"\bEQA?[A-Za-z0-9_\-]{30,}\b", t):
        found.add(m.group(0))

    return found

def normalize_contracts(raw_set):
    """
    ì£¼ì†Œ ë¬¸ìì—´ë§Œìœ¼ë¡œ ì²´ì¸ì„ 100% ë‹¨ì •í•˜ê¸´ ì–´ë µì§€ë§Œ,
    ê³µì§€/ë¬¸ì„œ ë¬¸ë§¥ì—ì„œ ë³´ì¡° í‚¤ì›Œë“œë¡œ ì²´ì¸ ë¼ë²¨ë§ì„ ì‹œë„í•  ìˆ˜ ìˆìŒ.
    ì—¬ê¸°ì„  ì•Œë¦¼ í¬ë§·ì„ ìœ„í•´ ë‹¨ìˆœ ë¼ë²¨ placeholderë§Œ ì¤€ë¹„.
    """
    # ì‹¤ì œ ë°°í¬ ì „ì—, ê³µì§€ ë³¸ë¬¸ ì£¼ë³€ ë¬¸êµ¬(â€œon BNB Chainâ€, â€œon Baseâ€ ë“±)ë¡œ ë¼ë²¨ë§ ê°•í™” ê¶Œì¥
    return {"Unknown": sorted(raw_set)} if raw_set else {}

def format_contract_lines(contracts_by_chain):
    if not contracts_by_chain:
        return "Contracts: (not provided)"
    parts = []
    for chain, addrs in contracts_by_chain.items():
        for addr in addrs:
            # ì²´ì¸ë³„ ìµìŠ¤í”Œë¡œëŸ¬ê°€ ìˆìœ¼ë©´ ë§í¬, ì—†ìœ¼ë©´ ìƒëµ
            if chain in EXPLORERS:
                parts.append(f"{chain}: {addr} [{EXPLORERS[chain].format(addr=addr)}]")
            else:
                parts.append(f"{chain}: {addr}")
    return " | " + " ; ".join(parts) if parts else " | Contracts: (not provided)"

def process_once(seen_ids):
    articles = fetch_new_listing_articles()
    new_hits = []
    for art in articles:
        code = art.get("code")  # detail code
        title = art.get("title", "")
        id_ = art.get("id") or code or title
        slug = art.get("slug")
        if id_ in seen_ids:
            continue
        if is_noise(title):
            continue
        if not is_alpha_new_listing(title):
            continue

        link = build_announcement_link(code, slug)
        # ì„¸ë¶€ HTMLì„ ì—´ì–´ í† í°ëª…/í‹°ì»¤, ê³µì‹ ë§í¬, ì»¨íŠ¸ë™íŠ¸ ì¶”ì¶œ
        html_text = get_article_detail_html(link)
        x_handle, contracts = extract_official_links_and_contracts(html_text)

        # ì œëª©ì—ì„œ í† í°ëª…/í‹°ì»¤ ì¶”ì¶œ (ì˜ˆ: â€œEVAA (EVAA) Will Be Available â€¦â€)
        m = re.search(r"([A-Za-z0-9\.\-_ ]+)\s*\(([A-Z0-9\-]{2,15})\)", title)
        if m:
            token_name = m.group(1).strip()
            ticker = m.group(2).strip()
        else:
            token_name = title
            ticker = "N/A"

        # ì•Œë¦¼ êµ¬ì„±
        x_display = f" | X: {x_handle}" if x_handle else ""
        contract_line = format_contract_lines(contracts)
        msg = f"ğŸš¨ Alpha listing: {token_name} ({ticker}) â€” {link}{x_display}{contract_line}"
        new_hits.append((id_, msg))

    for id_, msg in reversed(new_hits):  # ì˜¤ë˜ëœ ê²ƒë¶€í„° ì•Œë¦¼
        tg_send(msg)
        seen_ids.add(id_)
    return seen_ids, len(new_hits)

def main_loop():
    if not BOT_TOKEN or not CHAT_ID:
        raise SystemExit("TELEGRAM_BOT_TOKEN / TELEGRAM_CHAT_ID ê°€ .envì— í•„ìš”í•©ë‹ˆë‹¤.")
    seen = load_seen()
    tg_send("âœ… Alpha Listing Monitor started (15m interval).")
    while True:
        try:
            seen, cnt = process_once(seen)
            if cnt == 0:
                # í•„ìš”í•˜ë©´ ì¡°ìš©íˆ ëŒ€ê¸°. ì•„ë˜ ì¤„ì„ ì£¼ì„ í•´ì œí•˜ë©´ 'ì‹ ê·œ ì—†ìŒ' ë©”ì‹œì§€ë¥¼ ì£¼ê¸°ì ìœ¼ë¡œ ë³´ëƒ„
                # tg_send("No new Alpha listings yet.")
                pass
            save_seen(seen)
        except Exception as e:
            tg_send(f"âš ï¸ Monitor error: {e}")
        time.sleep(INTERVAL)

if __name__ == "__main__":
    main_loop()
